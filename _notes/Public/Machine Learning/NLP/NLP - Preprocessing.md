---
title: NLP - Preprocessing
feed: hide
---

출처 - [딥 러닝을 통한 자연어 처리 입문](https://wikidocs.net/book/2155)
# Tokenization

-   주어진 corpus에서 token이라는 단위로 나누는 작업
-   토큰의 단위가 상황에 따라 다르지만, 일반적으로는 의미 있는 단위로 토큰을 정의한다.

## Word Tokenization

-   토큰의 기준을 단어로 하는 경우, word tokenization이라 한다.
    -   단어는 단어 단위 외에 단어구, 의미를 갖는 문자열로도 간주한다.
-   punctuation과 같은 문자를 제외시킨다고 하자. (. , ? ; ! 등의 기호)
    -   가장 기본적인 토큰화 작업을 통해 다음과 처리될 수 있다.
	```
	input: Time is an illusion. Lunchtime double so!
	output: "Time", "is", "an", "illustion", "Lunchtime", "double", "so"
	```
	-   보통의 토큰화 작업은 단순히 구두점 / 특수문자를 전부 제거하는 정제 작업으로 해결되지 않는다.
	    -   특수 문자의 제거로 인해 토큰이 의미를 잃어버리는 경우가 발생한다.
	    -   한국어는 띄어쓰기만으로는 단어 토큰을 구분하기 어렵다.
-   토큰화를 진행하면서 예상하지 못한 경우가 있어 토큰화의 기준을 생각해봐야 하는 경우가 발생한다.
    -   이러한 기준은 어떤 용도로 사용할 것인지에 따라 그 용도에 영향이 없는 기준으로 정한다.
    -   아포스트로피 등 단어에서 기준을 고려해야 할 필요가 있다.
- 토큰화에서 다음과 같은 점을 고려해야 한다.
    1.  구두점이나 특수 문자를 단순 제외해서는 안된다.
        -   정제 작업을 진행하다보면 구두점조차도 하나의 토큰으로 분류하기도 한다.
            -   마침표같은 경우 문자의 경계를 알 수 있는데 도움이 될 수 있다.
        -   단어 자체에 구두점을 갖고 있는 경우도 있고, 특수 문자가 어떠한 의미를 부여하는 경우가 있다.
    2.  줄임말과 단어 내에 띄어쓰기가 있는 경우 단어의 개수를 고려해야 한다.
        -   아포스트로피로 인해 여러개의 단어가 하나로 압축될 수도 있다.
        -   또한 하나의 단어에 중간에 띄어쓰기가 들어간 경우 하나의 토큰으로 봐야할 수 있따.
    -  표준 토큰화의 경우 다음과 같이 구분이 이루어지게 된다
	- 하이픈으로 구성된 단어는 하나로 유지한다.
	- 아포토스로피로 압축된 단어는 분리해준다.

## Sentence Tokenization
-   갖고있는 corpus 내에서 문장 단위로 구분하는 작업
    -   때로는 sentence segmentation이라고도 한다.
-   갖고 있는 corpus가 정제되어있지 않으면 corpus는 문장 단위로 구분되어 있지 않을 수 있다.
    -   이를 사용하는 용도에 맞게 문장 토큰화가 필요할 수 있다.
-   단순히 마침표, 느낌표, 물음표 등의 구두점으로 구분하는 것은 정확하지 않다.
    -   마침표는 명확한 구분자의 역할을 하지 못할 수 있기 때문이다.
    - 사용하는 corpus가 어떤 언어인지, 또는 특수문자들이 어떻게 사용되는지에 따라 직접 규칙들을 정의할 수 있다.

- 한국어는 영어와 달리 띄어쓰기로만 토큰화를 하기 부족하다.
    -   한국어의 경우 띄어쓰기의 단위가 어절인데, 한국어가 조사 / 어미 등을 붙여 쓰는 교착어라 어절 토큰화랑 단어 토큰화가 다르다.
    -   교착어인 한국어는 다음과 같은 특성이 있다.
        -   조사라는 것이 존재한다.
            
            -   같은 단어임에도 서로 다른 조사가 붙어 다른 단어로 인식이 될 수 있다.
                
            -   형태소라는 개념을 반드시 이해해야 한다.
                
                -   형태소는 뜻을 가진 가장 작은 말의 단위를 의미한다.
                    -   자립 형태소 : 접사 / 어미 / 조사와 상관 없이 자립하여 사용할 수 있는 형태소
                        -   그 자체로 단어가 될 수 있다.
                        -   체언, 수식언, 감탄사 등이 있다.
                    -   의존 형태사 : 다른 형태소와 결합하여 사용되는 형태소
                        -   접사, 어미, 조사, 어간을 의미한다.
            -   어절과 단어로 분해는 다음과 같은 차이가 발생하게 된다.
                ```
                문장 : 에디가 책을 읽었다
                
                어절 단위 토큰화 : ['에디가', '책을', '읽었다']
                
                형태소 분해 :
                자립 형태소 : 에디, 책
                의존 형태소 : -가, -을, 읽-, -었, -다
                ```
        -   띄어쓰기가 영어보다 잘 지켜지지 않는다.
            -   한국어 corpus에는 띄어쓰기가 틀렸거나 지켜지지 않는 경우가 많다.
            -   한국어는 영어권 언어보다 띄어쓰기가 어렵고, 잘 지켜지지 않는 경향이 있다.
                -   한국어의 경우 띄어쓰기가 지켜지지 않아도 글을 쉽게 이해할 수 있는 언어이기 때문이다.
-   품사에 따라서 단어의 의미가 달라질 수 있다.
    -   따라서 토큰화 과정에서 품사로 구분되기도 한다

# Cleaning and Normalization

-   tokenization 전후로 텍스트 데이터를 용도에 맞게 cleaning과 normalization이 이루어질 수 있어야 한다.
    -   cleaning : 갖고 있는 corpus로부터 노이즈를 제거한다
    -   normalization : 표현 방법이 다른 단어를 통합시켜 같은 단어로 만들어준다.
-   cleaning 작업은 tokenization 작업 이전에 방해가 되는 부분을 배제하기 위해 이루어지기도 하고, 이후에 여전히 남아 있는 노이즈를 제거하기 위해 지속적으로 이루어지기도 한다.
    -   완벽한 cleaning 자체가 어렵고, 일종의 합의점을 찾기도 한다
-   다음과 같은 과정으로 normalization / cleaning이 이루어진다.
    1.  규칙에 기반한 표기가 다른 단어들의 통합
        -   필요에 따라 직접 정규화 규칙을 만들어, 표기가 다른 단어들을 하나의 단어로 정규화하는 방법을 사용할 수 있다.
            -   USA와 US는 같은 의미를 가지므로, 하나의 단어로 정규화 할 수 있다.
    2.  대 / 소문자 통합
        -   영어권 언어에서 대 / 소문자를 통합하여 단어를 줄일 수 있다.
            -   대부분의 글은 소문자로 쓰이기 때문에, 소문자 변환 작업으로 이루어지게 된다.
            -   이때 단어와 명사는 구분될 수 있어야 한다.
        -   이러한 작업은 더 많은 변수를 사용하여 소문자 변환을 언제 사용할지 결정하는 머신 러닝 시퀀스 모델로 정확하게 진행시킬 수 있다.
            -   데이터에 따라 예외 사항을 크게 고려하지 않고, 모든 corpus를 소문자로 바꾸는 것이 더 실용적인 해결책이 될 수 있다.
    3.  불필요한 단어의 제거
        -   정제 작업에서 제거해야 하는 노이즈는 두가지 종류가 있다.
            -   자연어가 아니면서 아무 의미 없는 단어
            -   분석하고자 하는 목적에 맞지 않는 불필요 단어
        -   불필요한 단어를 제거를 제거하는 방법으로 다음이 존재한다.
            -   불용어 제거 → 아래에서 더 자세히 보자
            -   등장 빈도가 적은 단어
                -   너무 적게 등장해서 자연어 처리에 도움이 되지 않는 단어들이 존재한다.
            -   길이가 짧은 단어
                -   영어권 언어에서 사용 가능한 방법이다. 영어권 단어의 평균 길이는 6-7, 한국어는 2-3이다.
                -   한국어에서는 따라서 이러한 방법이 잘 적용되지 않을 수 있다.
    4.  정규 표현식
        -   얻어온 데이터에서 노이즈 데이터의 특징을 잡아낼 수 있다면, 정규 표현식을 통해 이를 제거할 수 있다.

## Stemming and Lemmatization

-   눈으로 봤을 때 서로 다른 단어이지만, 하나의 단어로 일반화 시킬 수 있다면 일반화시켜 단어의 수를 줄일 수 있다.
    -   빈도수를 기반으로 문제를 풀고자 하는 BoW 문제에서 자주 사용되는 방법이다.
    -   자연어 처리에서 normalization의 지향점은 갖고 있는 corpus의 복잡성을 줄이는 것이다.

## Lemmatization

-   단어들로부터 표제어를 찾아가는 과정이다.
    -   단어들이 다른 형태를 가지더라도, 그 뿌리 단어를 찾아 단어의 개수를 줄일 수 있는지 판단한다.
        -   Ex) are, am, is는 모두 be라는 뿌리 단어로 연결시킬 수 있다.
-   Lemmatization의 가장 섬세한 방법은 단어의 형태학적 파싱을 먼저 진행하는 것이다.
    -   형태소는 의미를 가진 가장 작은 단위를 의미한다.
    -   형태학이란 형태소로부터 단어를 만들어가는 학문을 의미한다.
    -   형태소의 종류로는 어간(stem)과 접사(affix)가 존재한다.
        -   어간 : 단어의 의미를 담고 있는 단어의 핵심 붑분
        -   접사 : 단어에 추가적인 의미를 주는 부분
    -   형태학적 파싱은 이 두 가지 구성 요소를 분리하는 작업이 된다.
        -   단어의 품사 정보를 알아야 정확한 결과를 얻을 수 있다.
-   표제어 추출은 문맥을 고려하여 수행했을 때, 해당 단어의 품사 정보를 보존한다.
    -   어간 추출을 수행한 결과는 품사 정보가 보존되지 않는다.
        -   어간 추출을 한 결과는 사전에 존재하지 않는 단어일 수 있다.

## Stemming

-   어간을 추출하는 작업
    -   형태학적 분석을 단순화한 버전이라고 볼 수도 있고, 정해진 규칙을 보고 단어의 어미를 자르는 어림짐작의 작업이라고 볼 수도 있다.
    -   섬세한 작업이 아니기 때문에 어간 추출 후에 나오는 결과 단어는 사전에 존재하지 않을 수 있다.
-   표제어 추출보다 일반적으로 빠르고, 포터 어간 추출기의 경우 정밀하게 설계되어 정확도가 높다.
    -   어간 추출을 하고자 한다면 가장 준수한 선택이 된다.
-   stemmer 알고리즘을 사용할 땐, 사용하고자 하는 corpus에 적용해보고, 적합한 알고리즘을 고르면 된다.

### In Korean

-   동사와 형용사는 어간과 어미의 결합으로 구성된다.
    -   어간 : 용언을 활용할 때, 원칙적으로 모양이 변하지 않는 부분.
        -   활용에서 어미에 선행하는 부분. 때론 어간의 모양도 바뀔 수 있다.
    -   어미 : 용언의 어간 뒤에 붙어 활용하면서 변하는 부분. 여러 문법적 기능 수행
-   어간이 어미를 취할 때, 어간의 모습에 따라 규칙 활용 / 불규칙 활용으로 나누어지게 된다.
    1.  규칙 활용
        -   어간이 어미를 취할 대 어간의 모습이 일정하다.
        -   어간에 어미가 붙기 전의 모습과 어미가 붙은 후의 모습이 같아, 규칙 기반으로 어미를 단순 분리하면 어간 추출이 된다.
    2.  불규칙 활용
        -   어간이 어미를 취할 때 어간의 모습이 바뀌거나 취하는 어미가 특수한 어미일 경우
        -   단순한 분리만으로 어간 추출이 불가능하고, 조금 더 복잡한 규칙을 필요로 한다.

## Stopword

-   갖고 있는 데이터에서 유의미한 단어 토큰만을 선별하기 위해서는 큰 의미가 없는 단어 토큰을 제거하는 작업이 필요하다.
    -   자주 등장하지만 분석을 하는 것에 있어서 큰 도움이 되지 않는 단어를 불용어라 한다..
-   개발자가 직접 정의할 수도 있고, 패키지로 사용할 수도 있다.
-   한국어에서 불용어를 제거하기 위해서, 토큰화 후 조사 / 접속사를 제거하는 방법이 있다.
    -   조사 / 접속사 분만 아니라 명사 / 형용사와 같은 단어들 중에서 불용어로 제거하고 싶은 단어가 생길 수 있다.
    -   결국 사용자가 직접 불용어 사전을 만들 수 있게 된다.
-   보편적인 리스트는 있으나, 절대적인 기준은 아니다.
	- [https://www.ranks.nl/stopwords/korean](https://www.ranks.nl/stopwords/korean) / [https://bab2min.tistory.com/544](https://bab2min.tistory.com/544)

# Regular Expression

-   정규 표현식을 이용해 특정 규칙이 있는 텍스트 데이터를 빠르게 정제할 수 있다.
    -   다음과 같은 문법으로 작성할 수 있다.
        
        특수 문자
        
        설명
        
        .
        
        한 개의 임의의 문자를 의미한다.
        
        ?
        
        앞의 문자가 존재할 수도 있고, 존재하지 않을수도 있다. (0개 또는 1개)
        
        *
        
        앞의 문자가 무한개 존재할 수도 있고, 존재하지 않을 수도 있다. (0개 이상)
        
        +
        
        앞의 문자가 최소 한 개 이상 존재한다.
        
        ^
        
        뒤의 문자열로 문자열이 시작된다.
        
        $
        
        앞의 문자열로 문자열이 끝난다.
        
        {숫자}
        
        숫자만큼 반복한다.
        
        {숫자1, 숫자2}
        
        숫자 1 이상 / 숫자 2 이하만큼 반복한다. ? / * / + 로 대체할 수 있다.
        
        {숫자, }
        
        숫자 이상만큼 반복한다.
        
        []
        
        대괄호 안의 문자들 중 한개의 문자와 매치한다. 이때 [a-z]같이 범위를 지정할 수 있다.
        
        [a-zA-Z]는 알파벳 전체를 의미하고, 문자열에 알파벳이 존재하면 매치된다.
        
        [^문자]
        
        해당 문자를 제외한 문자를 매치한다.
        
    -   역슬래시를 이용하여 자주 쓰이는 문자 규칙들이 존재한다.
        
        \
        
        역 슬래쉬 문자 자체를 의미한다.
        
        \d
        
        모든 숫자를 의미한다. [0-9]와 동일.
        
        \D
        
        숫자가 제외한 모든 문자를 의미한다. [^0-9]와 동일
        
        \s
        
        공백을 의미한다.
        
        \S
        
        공백을 제외한 문자를 의미한다.
        
        \w
        
        알파벳 또는 숫자를 의미한다.
        
        \W
        
        알파벳 또는 숫자가 아닌 문자를 의미한다.
        

# Integer Encoding

-   컴퓨터는 숫자를 텍스트보다 잘 처리하기 때문에, 자연어 처리에서 텍스트를 숫자로 바꾸는 여러가지 기법들이 있다.
    
-   단어에 정수를 부여하는 방법으로 다음과 같이 존재한다.
    
    -   단어를 빈도수 순으로 정렬한 단어 집합을 만들고, 빈도수가 높은 순서대로 차례로 낮은 숫자부터 정수룰 부여한다.
    -   다음과 같이 분리되는 것을 확인할 수 있다.
    
    ```
    원문 : "A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain."
    
    문장 토큰화 결과 : 
    ['A barber is a person.', 'a barber is good person.',
    'a barber is huge person.', 'he Knew A Secret!',
    'The Secret He Kept is huge secret.', 'Huge secret.',
    'His barber kept his word.', 'a barber kept his word.',
    'His barber kept his secret.',
    'But keeping and keeping such a huge secret to himself was driving the barber crazy.',
    'the barber went up a huge mountain.']
    
    문장 토큰화에 대한 단어 토큰화 결과 : 
    [['barber', 'person'], ['barber', 'good', 'person'],
    ['barber', 'huge', 'person'], ['knew', 'secret'],
    ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], 
    ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], 
    ['barber', 'kept', 'secret'], 
    ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], 
    ['barber', 'went', 'huge', 'mountain']]
    
    단어 집합의 빈도수 :
    {'barber': 8, 'person': 3, 'good': 1, 'huge': 5, 'knew': 1, 'secret': 6,
    'kept': 4, 'word': 2, 'keeping': 2, 'driving': 1, 'crazy': 1,
    'went': 1, 'mountain': 1}
    ```
    
    -   다음과 같이 dict 구조로 단어를 키, 빈도수를 value로 정의할 수 있다.
        -   이후 이를 많이 나온 단어 순으로 정렬하여, 중요한 상위 단어만 뽑아낼 수 있다.
-   정수 임베딩은 여러가지 도구들을 통해 수행할 수 있고, 이건 직접 가서 확인해보세요.
    
    -   [https://wikidocs.net/31766](https://wikidocs.net/31766)

# Padding

-   자연어 처리를 하다보면 각 문장은 서로 길이가 다를 수 있다.
    
    -   기계는 길이가 전부 동일한 문서들에 대해서는 하나의 행렬로 보고, 한번에 묶어 처리할 수 있다.
    -   병렬 연산을 위해 여러 문장의 길이를 임의로 동일하게 맞춰줄 수 있다.
-   다음과 같이 문장의 임베딩 값이 주어질때, 패딩의 결과로 다음과 같이 출력될 수 있다.
    
    ```
    문장 :
    [[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2],
    [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]
    
    패딩 결과 :
    array([[ 1,  5,  0,  0,  0,  0,  0],
           [ 1,  8,  5,  0,  0,  0,  0],
           [ 1,  3,  5,  0,  0,  0,  0],
           [ 9,  2,  0,  0,  0,  0,  0],
           [ 2,  4,  3,  2,  0,  0,  0],
           [ 3,  2,  0,  0,  0,  0,  0],
           [ 1,  4,  6,  0,  0,  0,  0],
           [ 1,  4,  6,  0,  0,  0,  0],
           [ 1,  4,  2,  0,  0,  0,  0],
           [ 7,  7,  3,  2, 10,  1, 11],
           [ 1, 12,  3, 13,  0,  0,  0]])
    
    ```
    
    -   이때 패딩의 결과는 가장 긴 문장의 길이로 맞춰지게 된다.
        -   길이가 7보다 짧은 문장은 전부 숫자 0이 뒤에 붙은 것을 확인할 수 있다.
            -   기계는 이들을 하나의 행렬로 보고, 병렬 처리를 수행할 수 있다.
            -   0은 아무런 의미가 없고, 따라서 무시하게 된다.
                -   이렇게 데이터의 shape를 조정하는 방법을 제로 패딩이라고 한다.
-   케라스로 패딩 진행하면 앞을 0으로 채운다. 이건 옵션으로 바꿀 수 있다.
    
-   여튼 병렬 처리 등으로 연산을 수행하기 위해 패딩을 사용하곤 한다.
    

# One-Hot Encoding

-   원 핫 인코딩은 숫자로 단어를 표현하는 많은 기법 중 가장 기본적인 표현 방법이다.

## Word Vocabulary

-   단어 집합은 서로 다른 단어들의 집합이다.
    -   단어의 변형 형태 (book / books)도 다른 형태의 단어로 간주한다.
-   단어 집합에 있는 단어들을 통해 문자를 숫자, 벡터로 바꾸는 원 핫 인코딩으로 바꾸어 보자.
-   원-핫 인코딩을 위해서는 단어 집합을 만들 수 있어야 한다.
    -   텍스트의 모든 단어를 중복을 허용하지 않고 모아두면 단어 집합이 된다.
    -   이 단어 집합에 고유한 정수를 부여하는 정수 인코딩을 진행한다.
        -   텍스트에 단어가 총 5000개 존재하면, 단어 집합의 크기는 5000이다.
-   원 핫 인코딩은 단어 집합의 크기를 벡터의 차원으로 하고, 표현하고 싶은 단어의 인덱스에 1의 값을 부여한다.
    -   다른 인덱스에는 0을 부여한다.
-   다음과 같은 두 가지 과정으로 정리할 수 있다.
    1.  정수 인코딩을 수행한다.
        -   각 단어에 고유한 정수를 부여한다.
    2.  표현하고 싶은 단어의 고유한 정수를 인덱스로 간주하여, 해당 위치에만 1을 부여한다.
        -   다른 단어의 인덱스에는 0을 부여한다.
-   다음과 같은 한계점을 가지고 있다
    -   단어의 개수가 늘어날수록, 벡터를 저장하기 위한 공간이 늘어난다.
        -   벡터의 차언이 늘어나고, 저장공간 측면에서 매우 비효율적인 표현 방법이 된다.
    -   단어의 유사도를 표현하지 못한다.
        -   검색 시스템 등에서 문제가 될 수 있다.
        -   이러한 단점을 해결하기 위해 단어의 잠재 의미를 반영하여 다차원 공간에 벡터화하는 방법이 있다.
            -   카운트 기반의 벡터화 방법인 LSA / HAL
            -   예측 기반의 벡터화 NNLM / RNNLM / Word2vec / FastTexgt
            -   두가지 모두 사용하는 GloVe
            - 