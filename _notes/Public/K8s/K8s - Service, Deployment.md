---
title: K8s - Service, Deployment
feed: hide
---
## Deployment

### Using kubectl to Create a Deployment
- 쿠버네티스 클러스터 올려봤으면, 컨테이너로 만든 앱을 올려볼 수 있다. [[K8s - Clusters]]
	- 이를 위해서는 deployment configuration을 해보면 좋다.
- Deployment는 쿠버네티스가 어플리케이션의 인스턴스를 어떻게 조정할 것인지를 설정한다.
- Deployment를 만들면, Control Plane이 어플리케이션 인스턴스를 각 노드에 동작시키도록 스케줄링한다.
- 어플리케이션 인스턴스가 만들어지면, Deployment 컨트롤러가 인스턴스를 모니터링한다.
	- Node에서 인스턴스가 내려가거나 삭제되면, 컨트롤러가 클러스터의 다른 노드로 자동으로 복구한다.
	- 클러스터에 대한 문제 / 점검에 대한 자가 복구 매커니즘을 지닌다.
- 예전의 아키텍처는 이러한 자가 복구 능력이 부족했지만, 쿠버네티스는 합니다.![](https://d33wubrfki0l68.cloudfront.net/8700a7f5f0008913aa6c25a1b26c08461e4947c7/cfc2c/docs/tutorials/kubernetes-basics/public/images/module_02_first_app.svg)
- Deployment는 Kubectl 이용해서 할 수 있다.
	- Kubectl은 ubernetes API로 클러스터와 소통한다.
- Deployment를 생성하면, 컨테이너 이미지와 레플리카의 개수를 정의해야 한다.
	- 이러한 정보를 deployment 업데이트하면서 반영할 수 있다.
	- NGINX에 올라간 튜토리얼을 따라해보면서 한번 해보자.
### Deployment strategy
- 전략이 존재한다.
	- 하드웨어를 돌리고 있을 때, 새로운 버전을 배포할 때 이를 지운 후, 새 포드를 한번에 올린다.
	- recreate할 땐 순간적으로 다 내려가서 서비스가 정지된다.
- 그래서 롤링 업데이트라는 전략을 사용한다.
	- 여러개의 하드웨어를 돌릴때 순차적으로 하나씩 바꿔 끼우는 전략이다.
	- 서비스 중단은 발생하지 않지만, 동시에 두가지 버전이 존재할 수 있어 트래픽이 섞여 들어갈 수 있다.
- 블루&그린 배포나 카나리 배포는 지원 안된다.
	- 블루 / 그린 배포는 동시에 여러개를 띄우고, 트래픽의 레이블을 바꿔 틀어버린다.
		- 트래픽이 잠깐 끊기지만 버전을 유지할 수 있다.
		- deployment를 두개 만들고, selector로 순간적으로 바꾸도록 하는 것이다.
	- 카나리 배포는 새 버전까지 추가해서 테스트하면서 트래픽에 문제가 없는지 확인한 후, 새 버전으로 밀어버리는 전략이다.
		- 테스트 용도로 사용되는 전략이다.
		- 이것도 새롭게 deployment 올리고 selector 같이 물려서 가능하도록 한다.
- 쿠버네티스에는 istio라는 플러그인이 있고, 여기에는 프록시라는 개념이 있다.
	- 쿠버네티스에 가상 네트워크 레이어를 만들어 여러가지 배포 전략도 채용할 수 있다.
	- 멀티 쿠버네티스를 지원해서, 엔드 포인트로 트래픽이 들어오면 멀티 쿠버네티스로 카나리 배포 등을 할 수 있다.
- deployment는 롤링 업데이트 기능이 있다.
	- 배포 잘못되었을때 복구 할 수 있다는 점이 장점이다.

## Service
- 서비스는 자동으로 DNS 이름이 할당된다.
	- 쿠버네티스 자원 ip 계속 바뀌어서 정적으로 호출할 수 있는 엔드 포인트가 없다.
	- 따라서 DNS 서비스로 이름이 등록된다.
		- {서비스명}.{네임스페이스 이름}.svc.cluster.local 형태로 만들어진다.
	- 밖에서는 안보인다.
		- external ip 주소 할당해서 사용할 수 있다.
			- 이 ip를 사용해 쿠베 서비스에 갖다 붙일 수 있다.
- 서비스를 만들 때, 하나의 서비스에 멀티 포트를 부여할 수 있다.
	- 사용 포트로 트래픽을 받아 타겟 포트로 포워딩 한다.
	- 여러개의 ip 포워딩 룰을 만들 수 있다.
- 이때 session appinity를 설정할 수 있다.
	- 라운드 로빈은 순차적으로 트래픽을 배포한다.
	- 이 반대 방식이 세션 어피니티다.
		- 일종의 해쉬값을 사용하는 방식으로, 클라우드 ip 해쉬값 보고 같은걸로 보낸다.
		- 클라우드가 다른 포드로 붙을 수 있는데, 이러면 같은 클라우드는 같은 서버로 붙는다.
		- 세션을 유지하고 shared 메모리로 써도 좋지만, 복잡하지 않을 땐 이렇게 사용하는 것이 좋다.
		- 같은 유저는 같은 부분을 조회할 확률이 높아, 캐시의 hit을 높이기 위해서도 사용할 수 있다.
	- 아니면 external ip를 지정할 수 있다.

- 서비스는 여러개의 타입이 존재한다.
###  ClusterIP
- 생성하면 이게 기본이다.
- 클러스터 내부의 IP를 노출시킨다. 클러스터 내부에서만 접근이 가능하도록 한다

### Load Balancer
- VPC가 있고 내부에 쿠베가 있을 때, 클러스터 ip를 만들면 클러스터 ip로 만든 쿠베는 프라이빗 ip로만 접속할 수 있다.
	- 쿠베 외부에서 접속이 안된다.
	- 로드밸런서로 가면 로드밸런서가 public ip를 갖고, 따랴서 vpc에서 접속이 가능해진다.
	- 인터넷에도 접속이 가능해진다.
		- 이게 표준 로드밸런서이다.
- 클라우드 벤더들 보면 internal ip 쓰는 로드밸런서 있다.
	- 이건 VPC IP를 쓰는 것이다.
		- VPC 내부에선 접근 가능하지만, 외부에서는 접근할 수 없다.
- 접근 가능한 ip 대역을 클러스터 ip, vpc ip, 공용 ip 3가지로 나누어 생각해주어야 한다.
- 구글 같은 경우 VPC Native IP라는 모드가 있다.
	- 이렇게 클러스터를 생성하면 VPC 내부에 쿠버네티스가 생성되었을 때 쿠버네티스 ip가 vpc ip 대역을 사용하게 된다.
		- vpc 내부에서 쿠버로, 쿠버에서 vpc로 호출이 가능하다.
	- vpc native로 사용하지 않은 경우 쿠버네티스가 있을 때, 서비스를 통해서만 볼 수 있다.
		- NAT를 설치하여 확인할 수 있다.
	- 쿠베는 집적도가 되게 높아서 pod를 많이 띄우고, ip를 많이 소모하게 된다.
- External Name이라는 타입이 있다.
	- 외부의 IP로 연결해준다.
		- 코딩할 때 테스트를 위해 하드 코딩으로 부르게 만들 수 있고, 이럴 때 external name 사용한다.
### NordPort
- 서비스를 선택된 노드와 같은 포트에 있는 것에만 expose한다.
	- 실제 노드가 public port를 열어 여기서 라우팅을 한다.
- ClusterIP의 수퍼셋으로, 서비스를 클러스터 외부에서 접속할 수 있도록 한다.
- 실제 로드 밸런서를 사용할 수 없을 때 사용하지만, 권장하지 않는 방법이다.
	- 노드가 public ip를 갖고 외부로 노출되고, 따라서 보안적으로 좋지 않다.
	- 웬만하면 로드밸런서 통해서만 트래픽 받도록 설계해야 한다.

### Headless Service
- 호출을 할 수 없는 서비스이다.
- nslookup보면 연결된 서비스들 나온다.
	- headless 서비스에 사용하는 서비스 바인딩 해놓으면, 연결되어 있는 pod 정보를 확인할 수 있다.
- hashicop consul등이 자주 사용되는 서비스이다.
	- 이러한 것들이 서비스 디스커버리 메커니즘이다.

## Probes
- Liveness Probes
	- 서비스가 죽었는지 확인한다.
		- 죽은 것 같으면 kubelet이 재시작시킨다.
- readiness probe
	- 서비스 올리고 응답 안되면 서비스 리스트에서 빼고 올라올때까지 기다려준다.
	- 응답 오면, 서비스의 로드 밸런서에 넣는다.
- 이러한 probe를 구현하기 위해 http를 가장 자주 사용하고, ssh도 사용은 한다.
	- http가 멀쩡히 오면 정상 작동하고, 아니면 죽은걸로 치는 것이다.
	- 아니면 shell script 사용할 수 있다.
		- healthy 파일 있으면 멀쩡한걸로, 아니면 죽은걸로 생각하는 것이다.
		- heartbeat를 몇초부터 확인할 것인지, 몇초 단위로 확인할 것인지도 올려야 한다.
- 이외에도 여러가지 probe 기법들이 존재한다.
	- timeoutSeconds는 몇초 안에 답변 안우면 죽은것으로 판단할 것인지 결정한다.
	- failureThreshold는 몇번 답변 제대로 안오면 죽은것으로 판단할 것인지 결정한다.
- 따로 디렉토리 만들어서 healthy check policy 만들 수 있다.
	- pod들을 만들어서 앞에 서비스를 여러개 붙였다고 하자.
		- ip가 다를 것인데, 어떻게 처리할 것인가?
			- 새롭게 엔드포인트를 하나 더 묶어주는데 이렇게 수행하는 것이 ingress이다. 일종의 로드 밸런서이다.
			- 밖을 묶는 로드밸런서는 ALB / 안을 묶는 로드밸런서는 ELB로 찍힌다.
		- 서비스를 구성하게 되면 Ingress를 붙여 엔드 포인트를 뽑아주어야 한다.
- Ingress는 L7 로드밸런서인데, HTTP이므로 HTTPS로도 커뮤니케이션을 할 수 있다.


## Using a Service to Expose Your App
### Overview of Kubernetes Services
- 쿠버네티스의 Pod는 라이프사이클을 가지고 있다.
	- 워커 노드 죽으면 안에 있던 Pod 같이 죽는다.
	- ReplicaSet이 클러스터에서 새로운 Pod 생성하여 어플리케이션 작동하도록 할 수 있다.
		- 죽어서 다른 걸로 바꾸거나 해도 크게 문제되지 않는다.
- 쿠버네티스의 각 Pod는 unique한 ip 주소를 갖고 있다.
	- 같은 노드에 들어있는 Pod 또한 그러하다.
- 쿠버네티스의 서비스는 Pod의 논리적인 집합과 이를 접근하는 정책에 대한 추상화이다.
	- 서비스는 의존적인 Pod를 루즈한 couplingㅡㅇ로 정의할 수 있다.
- 서비스는 YAML이나 JSON등의 문서로 표현될 수 있다.
- Pod 집합은 서비스로 LabelSelector를 통해 타겟팅 된다.
- 각 Pod가 Unique한 ip 주소를 갖고 있지만, 클러스터 외부에서 서비스 없이 이를 사용할 수 없다.
	- 서비스는 어플리케이션이 트래픽을 수용할 수 있도록 한다.
- 서비스는 ServiceSpec에 type을 정의하여 여러가지 방법으로 노출 시킬 수 있다.
	- ClusterIP
	- NodePort
	- LoadBalancer
	- ExternalName
		- CNAME 레코드와 값을 통해 서비스의 컨텐츠를 externalName 필드로 맵핑한다.

- 서비스의 유즈 케이스에서 selector를 정의하지 않는 경우가 있다.
	- 이런 경우 대응되는 Endpoint 객체를 만들지 않는다.
	- 이렇게 해서 유저가 직접 서비스로 가는 endpoint를 정의할 수 있게 된다.
	- 또한 ExternalName 타입을 strict하게 사용하면 셀렉털르 사용하지 않는다.

### Services and Labels
- 서비스는 Pod들의 집합에서 트래픽을 연결한다.
- 서비스는 쿠베 내부에서 어플리케이션에 영향을 주지 않고 Pod이 죽고 복제되는 상황을 추상화한다.
	- 의존적인 Pod를 발견하고 라우팅하는 것은 쿠버네티스 서비스에 의해 이루어진다.
- 서비스는 label과 selector를 이용해 Pod을 찾는다.
	- Label은 키/값 페어로 연결되어 있는 객체이며, 여러가지 용도로 사용될 수 있다.
		- 개발 / 테스트 / 프로덕션을 위한 객체 디자인
		- 버전 태그
		- 객체 분류