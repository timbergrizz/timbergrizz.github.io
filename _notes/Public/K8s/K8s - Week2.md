---
title: K8s - Week2
feed: hide
---

## 쿠버네티스 IDE
- Service는 굳이 service라고 이름 붙일 필요 없다.
- Cloud Code 쓰면 좋다.
	- 자등으로 파일 구분 된다.
	- 서비스명도 동일하게 작동한다.
- 쿠베는 어느정도 하면 파일 어떻게 묶는지 / 리소스 네이밍 어케 해야할지 고민이 많이 된다.
	- 이러한 툴 사용하면 좋다.

## DaemonSet
- 노드를 모니터링하는 팟으로 노드당 하나만 작동한다
	- 레이블 셀렉터를 통해 작동하지 않도록 할 수도 있다.

## Job
- Deployment는 올라가면 안내려온다.
- Job이 끝나면 수명이 끝난다.
	- Cron Jobs은 스케줄링되어 작동할 수 있다.
- Sequential run과 Parallel run이 존재한다.
	- 시퀀셜 런은 순차적으로 진행한다.
	- 패러럴 런은 여러개의 잡을 띄워서 작동시킬 수 있다.

## Namespace
- 굉장히 중요한 분야
	- 서비스를 분야 별로 나눈다고 할 수 있다.
	- 아마존은 account / 구글은 project라는 개념을 사용한다.
		- 큰 쿠버네티스 클러스터가 있을 때, 이를 namespace로 쪼개어 동작시킨다.
		- 그리고 쿼터를 정의하여 사용할 CPU, 메모리, 접근 권한 등을 지정하는 것이다.,
			- 팀 단위로 IAM을 지정해준다고 생각하면 좋다.

## Deployment
### Deployment strategy
- 전략이 존재한다.
	- 하드웨어를 돌리고 있을 때, 새로운 버전을 배포할 때 이를 지운 후, 새 포드를 한번에 올린다.
	- recreate할 땐 순간적으로 다 내려가서 서비스가 정지된다.
- 그래서 롤링 업데이트라는 전략을 사용한다.
	- 여러개의 하드웨어를 돌릴때 순차적으로 하나씩 바꿔 끼우는 전략이다.
	- 서비스 중단은 발생하지 않지만, 동시에 두가지 버전이 존재할 수 있어 트래픽이 섞여 들어갈 수 있다.
- 블루&그린 배포나 카나리 배포는 지원 안된다.
	- 블루 / 그린 배포는 동시에 여러개를 띄우고, 트래픽의 레이블을 바꿔 틀어버린다.
		- 트래픽이 잠깐 끊기지만 버전을 유지할 수 있다.
		- deployment를 두개 만들고, selector로 순간적으로 바꾸도록 하는 것이다.
	- 카나리 배포는 새 버전까지 추가해서 테스트하면서 트래픽에 문제가 없는지 확인한 후, 새 버전으로 밀어버리는 전략이다.
		- 테스트 용도로 사용되는 전략이다.
		- 이것도 새롭게 deployment 올리고 selector 같이 물려서 가능하도록 한다.
- 쿠버네티스에는 istio라는 플러그인이 있고, 여기에는 프록시라는 개념이 있다.
	- 쿠버네티스에 가상 네트워크 레이어를 만들어 여러가지 배포 전략도 채용할 수 있다.
	- 멀티 쿠버네티스를 지원해서, 엔드 포인트로 트래픽이 들어오면 멀티 쿠버네티스로 카나리 배포 등을 할 수 있다.
- deployment는 롤링 업데이트 기능이 있다.
	- 배포 잘못되었을때 복구 할 수 있다는 점이 장점이다.

## Service
- 서비스는 자동으로 DNS 이름이 할당된다.
	- 쿠버네티스 자원 ip 계속 바뀌어서 정적으로 호출할 수 있는 엔드 포인트가 없다.
	- 따라서 DNS 서비스로 이름이 등록된다.
		- {서비스명}.{네임스페이스 이름}.svc.cluster.local 형태로 만들어진다.
	- 밖에서는 안보인다.
		- external ip 주소 할당해서 사용할 수 있다.
			- 이 ip를 사용해 쿠베 서비스에 갖다 붙일 수 있다.
- 서비스를 만들 때, 하나의 서비스에 멀티 포트를 부여할 수 있다.
	- 사용 포트로 트래픽을 받아 타겟 포트로 포워딩 한다.
	- 여러개의 ip 포워딩 룰을 만들 수 있다.
- 이때 session appinity를 설정할 수 있다.
	- 라운드 로빈은 순차적으로 트래픽을 배포한다.
	- 이 반대 방식이 세션 어피니티다.
		- 일종의 해쉬값을 사용하는 방식으로, 클라우드 ip 해쉬값 보고 같은걸로 보낸다.
		- 클라우드가 다른 포드로 붙을 수 있는데, 이러면 같은 클라우드는 같은 서버로 붙는다.
		- 세션을 유지하고 shared 메모리로 써도 좋지만, 복잡하지 않을 땐 이렇게 사용하는 것이 좋다.
		- 같은 유저는 같은 부분을 조회할 확률이 높아, 캐시의 hit을 높이기 위해서도 사용할 수 있다.
	- 아니면 external ip를 지정할 수 있다.

- 서비스는 여러개의 타입이 존재한다.
- ClusterIP
	- 생성하면 이게 기본이다.
- Load Balancer
- VPC가 있고 내부에 쿠베가 있을 때, 클러스터 ip를 만들면 클러스터 ip로 만든 쿠베는 프라이빗 ip로만 접속할 수 있다.
	- 쿠베 외부에서 접속이 안된다.
	- 로드밸런서로 가면 로드밸런서가 public ip를 갖고, 따랴서 vpc에서 접속이 가능해진다.
	- 인터넷에도 접속이 가능해진다.
		- 이게 표준 로드밸런서이다.
- 클라우드 벤더들 보면 internal ip 쓰는 로드밸런서 있다.
	- 이건 VPC IP를 쓰는 것이다.
		- VPC 내부에선 접근 가능하지만, 외부에서는 접근할 수 없다.
- 접근 가능한 ip 대역을 클러스터 ip, vpc ip, 공용 ip 3가지로 나누어 생각해주어야 한다.
- 구글 같은 경우 VPC Native IP라는 모드가 있다.
	- 이렇게 클러스터를 생성하면 VPC 내부에 쿠버네티스가 생성되었을 때 쿠버네티스 ip가 vpc ip 대역을 사용하게 된다.
		- vpc 내부에서 쿠버로, 쿠버에서 vpc로 호출이 가능하다.
	- vpc native로 사용하지 않은 경우 쿠버네티스가 있을 때, 서비스를 통해서만 볼 수 있다.
		- NAT를 설치하여 확인할 수 있다.
	- 쿠베는 집적도가 되게 높아서 pod를 많이 띄우고, ip를 많이 소모하게 된다.
- External Name이라는 타입이 있다.
	- 외부의 IP로 연결해준다.
		- 코딩할 때 테스트를 위해 하드 코딩으로 부르게 만들 수 있고, 이럴 때 external name 사용한다.
- NordPort가 있다.
	- 실제 노드가 public port를 열어 여기서 라우팅을 한다.
	- 실제 로드 밸런서를 사용할 수 없을 때 사용한다.
	- 권장하지 않는 방법이다.
		- 노드가 public ip를 갖고 외부로 노출되고, 따라서 보안적으로 좋지 않다.
		- 웬만하면 로드밸런서 통해서만 트래픽 받도록 설계해야 한다.
- Headless Service
	- 호출을 할 수 없는 서비스이다.
	- nslookup보면 연결된 서비스들 나온다.
		- headless 서비스에 사용하는 서비스 바인딩 해놓으면, 연결되어 있는 pod 정보를 확인할 수 있다.
- hashicop consul등이 자주 사용되는 서비스이다.
	- 이러한 것들이 서비스 디스커버리 메커니즘이다.

### Probes
- Liveness Probes
	- 서비스가 죽었는지 확인한다.
		- 죽은 것 같으면 kubelet이 재시작시킨다.
- readiness probe
	- 서비스 올리고 응답 안되면 서비스 리스트에서 빼고 올라올때까지 기다려준다.
	- 응답 오면, 서비스의 로드 밸런서에 넣는다.
- 이러한 probe를 구현하기 위해 http를 가장 자주 사용하고, ssh도 사용은 한다.
	- http가 멀쩡히 오면 정상 작동하고, 아니면 죽은걸로 치는 것이다.
	- 아니면 shell script 사용할 수 있다.
		- healthy 파일 있으면 멀쩡한걸로, 아니면 죽은걸로 생각하는 것이다.
		- heartbeat를 몇초부터 확인할 것인지, 몇초 단위로 확인할 것인지도 올려야 한다.
- 이외에도 여러가지 probe 기법들이 존재한다.
	- timeoutSeconds는 몇초 안에 답변 안우면 죽은것으로 판단할 것인지 결정한다.
	- failureThreshold는 몇번 답변 제대로 안오면 죽은것으로 판단할 것인지 결정한다.
- 따로 디렉토리 만들어서 healthy check policy 만들 수 있다.
	- pod들을 만들어서 앞에 서비스를 여러개 붙였다고 하자.
		- ip가 다를 것인데, 어떻게 처리할 것인가?
			- 새롭게 엔드포인트를 하나 더 묶어주는데 이렇게 수행하는 것이 ingress이다. 일종의 로드 밸런서이다.
			- 밖을 묶는 로드밸런서는 ALB / 안을 묶는 로드밸런서는 ELB로 찍힌다.
		- 서비스를 구성하게 되면 Ingress를 붙여 엔드 포인트를 뽑아주어야 한다.
- Ingress는 L7 로드밸런서인데, HTTP이므로 HTTPS로도 커뮤니케이션을 할 수 있다.

## Configmap
- 우리가 환경 설정 파일 빼는데, 이렇게 환경 설정 파일 빼는걸 configmap이라 한다.
	- 운영 환경과 개발 환경 뺄 수 있고, 이를 환경 변수로 빼는걸 configmap이라 한다.
- deployment에서 key가 language인 것의 값을 ㅂ다아 넣어주는 형식으로 만들 수 있다.
	- 코드 내에서 환경변수 내에 있는 값을 읽어서 사용하는 방식으로 사용하게 된다.
	- 환경 변수가 여러개 존재할 수 있고, 파일 자체를 configmap으로 설정할 수 있다.
		- 이 파일은 환경 변수로 넘기는게 아니라, 마운트를 시켜 처리해버릴 수 있다.
		- 그럼 configmap의 파일이 파일 시스템에 묶어서 올라간다.
- 파일도 할 수 있고, 디렉토리도 올릴 수 있다.
	- 반드시 작성해야 하는 것 중 하나이다.
- ingress 쓸때 서비스에서 clusterIP나 LoadBalance 쓴다.
	- External Loadbalancer 쓰지 마라.
		- 이거 밖으로 빼는 방법은 좋지 않다.
		- internal type을 사용하는 것이 좋다.
- ingress로 로드밸런싱을 할 경우 굳이 load balance type으로 열 필요가 없다.

### Secret
- Configmap과 동일한데, configmap은 모든 값을 조회할 수 있다.
- 실제로 클라우드에서는 키 매니지먼트 서비스가 존재한다.
	- HSM이라는 모듈이 있는데, 보안키를 저장할 수 있다.
	- 이걸 사용하려면 접속하려는 키가 필요하다.
	- 쿠버네티스에서는 키가 KMS에 저장한다.
- config map은 etcd에 저장되는데, 이 노드가 탈취되면 다 털리는거다.
	- KMS는 쿠베 밖에 있어서 못턴다.
	- 그래서 AWS [EKS Secret]([https://docs.aws.amazon.com/secretsmanager/latest/userguide/security-encryption.html](https://docs.aws.amazon.com/secretsmanager/latest/userguide/security-encryption.html)) 같은걸 써서 중요한 키 등을 저장한다.

### Volume
- 볼륨은 디스크이다.
	- 네트워크 디스크 등이 있는데, 한가지 타임은 emptyDir이다.
		- Pod 띄우면 Pod 안에 로컬 디스크 공간을 볼륨으로 정의하는 것이다.
		- 컨테이너를 Pod에 2개 이상 띄울 수 있는데, 양쪽에 다 물리기 위해서 쓰는 것이다.
	- hostPath
		- 노드에 있는 디스크 공간을 공유한다.
			- Pod 밖의 데이터를 공유할 수 있도록 한다.
			- deployment로 컨테이너 다시 살아나는 경우 다른 노드의 디렉토리이다.
			- 일반적으로 잘 사용하지 않는다.
	- PersistenceVolume
		- 외부 스토리지를 물리는 방식이다.
		- 가장 일반적으로 사용한다.
		- 클라우드 컴퓨팅 환경에서 외부에 생성한 디스크는 원래 보이지 않는데, 이를 접근하도록 한다.
		- 하드웨어 디스크 등록했을 때, 이를 Pod로 연결할 수 있도록 하는 것을 PersistenceVolume이다.
			- 볼륨 모드를 보아야 한다.
		- 파일 시스템은 일반적으로 생각하는 그거다.
		- rawblock mode라는 것이 있따.
			- sfs같은 게 있는데, 이런걸 raw disk를 물려야 한다.
		- Reclaim Policy
			- 사용 끝나면 어떻게 할 것인가?
				- Delete : 디스크 지워버린다.
				- Retain : 디스크 남겨둔다.
				- Recycle : 디스크 지우지 않고 내용만 다 지운다.
					- 이러한 정책을 사용하는데는 크게 두가지 이유가 있다.
						- 디스크 가격 정책때문에 그럴 수 있다.
							- 최저 사용 시간등이 있을 수 있다.
						- 클라우드 자원은 무제한이 아니고, 프로비저닝 과정에서 경합이 발생하는 경우가 발생한다.
							- 이를 선점하기 위해서 사용할 수 있다.
		- Read-write policy
			- ReadWriteOnce : 한 파일만 읽고 쓰기만 가능하다.
			- ReadWriteMany : 여러번 읽고 쓰기가 가능하다.
		- 디스크는 상태가 있고, attach가 되면 bound가 된다.
			- 사용이 끝나면 release가 된다. 이때는 아직 사용 불가능한 상태고, 사용 가능한 상태가 되면 available로 바뀐다.
		- 디스크를 매번 직접 생성할 수 없다.
			- 이를 클라우드 설정에 명시하여 동적으로 프로비저닝할 수 있다.
			- 100메가짜리 디스크를 testtype으로 선언하는 것이다.
				- PersistentVolumeClaim 등으로 선언할 수 있다.

## StatefulSet
- Replica set으로 정의하면 볼륨은 하난데 pod는 여러개가 된다.
	- 여러개의 replica pod가 하나의 볼륨을 가리킨다.
- 디스크도 template로 하자고 나온 거이 stateful set이다.
- pod에 관한 정보도 있지만 volume에 대한 정보도 있다.
	- 이게 stateful set이 된다.
	- persistent volume도 하나씩 생긴다.
		- 생성이 되면 이름이 순차적으로 붇는다.
- 오토스케일로 스케일 아웃이 되어도 디스크는 살려둔다.
	- ordered ready가 있고, 이건 pod를 하나씩 띄운다.
	- slavenode가 마스터 노드에 한번에 붙으면 replication lag때문에 죽는다.
		- 하나씩 띄우면서 서버에 부담을 줄이는 방법을 사용할 수 있다.
- 클라우드 내에 main service로 rds등을 올리기 때문에, 일반적으로 쿠베 내에 DB를 직접 올리는 경우는 잘 없다.
	- 노드가 scale down등이 발생하면 restart가 되고, 이러면 신나게 되는 거다.
		- 굳이 이렇게 할 필요가 없다.
	- API는 업데이트가 잦고, DB는 업데이트를 안한ㄴ다. 그래서 쿠베를 안에서 돌릴 필요가 없다.
		- 개발 환경등을 만들 때는 이렇게 사용할 수 있다.
		- apt-get으로 helm 설치하면 db 깔린다.
			- 표준 환경으로 사용하기 위해서 사용한다.
	- 돌릴 필요가 없을 뿐이지 돌리는 경우들도 있다.
		- 쿠베 내부에서 DB 서비스 하는 업체들 많다.
		- Namespace 격리시키고, 권한 확실하게 줄 수 있어서 좋다.

여기까지가 기본이다. 지금까지 할게 신난다.

## Resource Management
- pod에 당연히 자원을 할당할 수 있다.
	- CPU / Memory 할당 되고, 네트워크는 아직 안될거다.
	- ms라는 단위를 사용해 CPU를 할당하고, 1000ms가 1CPU로 볼 수 있다.
- 쿠버네티스에서 한 자릿수대가 나올 수 있다.
	- CPU를 한개도 안 쓰는 것이고, 효율성이 엄청나게 올라게 된다.
- CPU에는 request와 limit이 있다.
	- request는 처음 할당되는 자원이고, limit은 최대 사용할 수 있는 자원이다.
	- 메모리도 limit까지 늘어날 수 있다.
- kubectl top 등의 명령을 이용해 자원의 사용을 모니터링을 할 수 있다.
	- namespace에 메모리와 cpu를 할당할 수 있다.
- request와 limit을 지정할 수 있다.
	- Pod 뜰 때 이거 안주는데, 안주면 기본값으로 들어간다.

- Overcommitted state
	- pod를 띄웠는데 limit의 총합 값이 서버의 총 물리적 메모리가 큰 경우이다.
		- 뜰때는 request가 커서 되는데, limit 찍어서 안되면 overcommited라 한다.
			- 이를 쿠베는 request만큼 cpu를 내리고, 램 부족하면 pod 리스타트 갈긴다.
			- 아니면 cpu 쓰로틀링 건다.
	- request와 limit 값 같이 줘서 이러한 상황을 발생하지 않도록 하는 것이 좋다.
- 개발과 운영 과정은 다르고, 이러한 점에서 할당하는 것이 굉장히 중요하다.
	- vertical autoscaling이 있는데, 필요한 만큼 늘리게 된다.
	- manual로 두면 최대까지 먹여볼 수 있고, 그걸로 production 가기 전에 부하 테스트 해서 성능을 설정할 수 있다.
		- 한계상황까지 부하를 주어야 테스트를 진행할 수 있는 것이다.
- [locust]([https://locust.io/](https://locust.io/))라는 부하 테스트 툴 사용할 수 있다.
	- 이거 러닝커브가 낮고 쿠베 내에서 실행할 수 있어서 좋다.
	- [부하 테스트 방법론]([https://bcho.tistory.com/787?category=75945](https://bcho.tistory.com/787?category=75945)) 여기서 확인해봐라.
	- [이거 보고]([https://bcho.tistory.com/1371](https://bcho.tistory.com/1371)) 테스트 진행할 수 있다.
- 사실은 키워드만 알면 된다.
	- 이 강의에서 집중해야 하는 것은 쿠버네티스 100% 이해하는 것이 아닌, 쿠베의 키워드에서 필요한 단어들을 이해하는 것이다.
	- 팁 같은걸 많이 노트해두면 좋다.
		- 쿠베 자체는 책 보면 다 된다.
		- 강의 learning path랑 팁을 참조하는 것이 좋다.

## Scheduling
- Node들이 있을 때, 어떤 Node로 보낼 것인가?
	- 이를 결정하는 것이 스케줄링이다.
- Taint : admin
	- 여기로 오지 마라는 의미이다.
		- Taint 걸린 Node에는 Pod 배포하지 않는다.
	- 이게 Admin이고, 여기로 배포하려면 Toleration이 필요하다.
		- 이 Pod는 이 노드로 배포되는 것을 허가받았다.
		- key=value:effect 형식으로 정의된다.
	- Taint에는 3가지 effect가 있다.
		- noschedule
			- taint 처리 된 상태에서 pod 배포하려면 새로 배포해야 한다.
			- 이미 들어간 pod는 영향을 끼치지 않는다.
		- noexecute
			- 기존 pod 있어도 toleration 없으면 다 쫓아낸다.
			- 의도적으로 node를 비워야 할 때가 있다.
				- VM 점검할 때 이거 다른데로 다 보내야 할 때 이런거 사용하면 좋다.
				- 보통은 NoSchedule을 많이 사용한다.
- Node를 일반적으로 여러 타입을 섞는다.
	- Node Pool이라 하는데, Node를 여러 종류로 가진 클러스터를 배포할 수 있다.
		- 이거 적절한 컴퓨팅 성능에 배포될 수 있도록 배치를 해주어야 한다.

- GCP 쿠베 중요한 것 중 하나가 글로벌 VPC라는 점이다.
	- 아마존 계정마다 하나의 리전만을 갖는데, 구글은 한 프로젝트에서 여러가지 리전을 삽입할 수 있다.
	- 쿠베 클러스터를 여러개의 리전을 걸쳐 만들 수 있다.
		- 글로벌 서비스 구축에 굉장히 편해진다.
	- AWS는 글로벌로 굴릴때 클러스터 한무 복제된다.
	- 로드밸런서도 특정 지점에 생성되는 것이 아닌, 전세계에 배포되고 가장 가까운 곳으로 배포된다.
		- 구글 전용 네트워크 망이라 존나빠르다.
		- AWS는 퍼블릭 망 타고 들어가는거라 그래도 구글보단 느리게 동작한다.
			- 이러한 아키텍처에서의 차이점이 존재한다.