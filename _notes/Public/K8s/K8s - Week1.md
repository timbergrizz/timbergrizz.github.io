---
title: K8s - Week1
feed: hide
---

-   한국은 아직도 vm 쓰는데가 있는데, 미국은 대부분 쿠버네티스를 사용한다
-   학슴의 목적은 learning path
    -   어떤 부분을 봐야할 지를 결정할 수 있어야 한다.
    -   쿠베 책 찾아보면 좋은게 많다. 어디서 써야하는지 / 어디서 쓰면 안되는지 이해하는 것이 중요한다.
        -   왜 공부하는지를 이해하면 좋을 것이다
    -   실 운영 환경 자체를 구성하는데 필요한 것을 이해하는 것이 좋다.
-   쿠버네티스를 설치하는 것은 거의 할 일이 없을 것이다.
    -   돈 주고 사는게 아니라 오픈소스 쿠베 가지고 환경 가지고 하는 사람들 외에는 설치할 일 없다.
    -   보통은 패키지성 쿠버네티스 사용하고, 이건 같이 인스톨해주기 때문에 신경쓸 필요 없다.
        -   나머지는 클라우드에서 쿠베 사용하고, 신경 쓸 필요 없다.
-   vm생성할 때, 클라우드에는 vm 종류가 어마어마하게 많다.
    -   일반적인 cpu 비용보다 싼 인스턴스가 있다.
        -   미세한 차이가 있다.
    -   그런 점에서 어떤 차이가 있고, 이걸 어떻게 다룰 수 있을지 생각해야 한다.

## 마이크로서비스

-   기술이 왜 생겨났는지 원인을 생각해보면 좋다.
    -   어떤 것이든 왜라는게 없으면 이해하기 어렵다.
-   소프트웨어 개발이 어떻게 변해왔는지 이해해야 한다.
    -   90년대에 it가 시작되고, 여러가지 인터넷 기업들이 나오고 인터넷이 보급된다.
    -   인터넷 기업들이 자신들의 기술을 풀어놓기 시작하고, 이게 오픈소스가 된다.
-   서블릿, JSP등의 기술들이 시장에 나오기 시작한다.
    -   독점권들이 많아 기술이 공부하기 쉽지 않았다.
    -   소프트웨어를 바탕으로 돈을 벌기 어려웠다.
-   스티브 잡스가 아이폰 들고나오면서 세상이 바뀌었다.
    -   개인 개발자가 돈을 벌 수 있도록 하게 된것이다.
    -   누구나 앱을 만들고 돈을 벌 수 있게 된 것이다.
        -   스타트업이 생기기 시작한다.
-   누구나 앱을 만들 수 있고, 하기 때문에, 새로운 기술을 빠르게 적용할 수 있어야 한다.
    -   기업형 어플리케이션은 시스템의 안정성 / 일관성이 중요했다.
    -   데이터가 깨지면 안된다.
    -   가용성 / integrity가 중요했다.
-   글로벌 서비스를 사용되면서 기존의 인프라를 바탕으로 서비스를 제공하기 어려워져 하둡 등의 연구되기 시작되었다.

### 속도형 아키텍처

-   기존에는 팀을 만들 때 기능 단위로 팀을 만들었다.
    -   부서가 기능 단위로 분리되고, 개발도 부서 단위로 이루어져 있다
        -   각 파트간의 협업이 어렵다.
-   애자일 컨셉과 비슷해지는데, 기능 단위가 아닌, 서비스 단위로 팀을 만들게 되었다.
    -   팀에 모든 유닛이 다 들어가게 된다.
        -   의사 결정이 팀 내부에서만 발생하고, 승인이 필요 없어지게 된다.
    -   프로덕트 오너는 그 프로덕트를 책임지는 책임자라고 할 수 있다.
        -   모든 것을 결정하게 된다.
    -   모놀리식 아키텍처에서 이런 문화를 적용하기 어렵다.
        -   각각의 컴포넌트를 개발하되 개발에 대한 종속성을 제거하여 속도를 높일 수 있도록 하는 것이다.
-   항상 좋은 기술은 아니다.
    -   계좌 이체에서 transaction이 발생한다고 하자.
        -   컴포넌트 분리되고 db 분리되면서 신나진다.
        -   입금, 출금 같이 발생해야 하는데 둘 중 하나 죽으면 신나진다.
    -   Data의 consistency가 중요하거나 빠르게 처리되어야 하는 경우 모놀리식 아키텍처가 사용되는 것이 좋다.
        -   MSA도 하나의 옵션일 뿐, 무지성 MSA 하시면 안된다.
-   컨웨이의 법칙
    -   소프트웨어 아키텍처의 구조는 소프트웨어를 구현하는 규칙을 따라간다.
        -   한국 / 인도 / 미국 / 캐나다가 팀을 이루고 있다고 하자.
        -   MSA 아키텍처를 도입해야 할 때, 팀의 수준에 따라 아키텍처를 분배하고 처리했다.
            -   시간대가 같은 팀은 잘 연결된다.
            -   시간대가 떨어지면 연결이 잘 안된다.
        -   이론적으로 아키텍처를 잘 구분했다고 하더라도 문제가 발생할 수 있다.
    -   컴포넌트를 어떻게 자르냐는 질문을 많이 받는데, 제일 모범적인 대답은 업무 별로 자르는 것이다.
        -   경험상 팀의 구조를 맞추어 설계하는 것이 맞다.
        -   컴포넌트를 만든 후 팀원들을 내려보내는 것은 아니다.
-   북스토어라는 곳에서 분리를 하는 것이다.
    -   기동형 / 속도를 높이는 개발형 프로세스를 설정했다.
        -   개발이 빨라져도 기동이 느려진다.
        -   운영과 개발을 합치기 위해 데브옵스가 등장했다.
            -   14년 전에 데브옵스 구분하는 걸 고민했다.
            -   데브옵스는 개발 / 운영팀을 합치는 과정이다.
    -   제대로 된 데브옵스는 뭐라 할 수 있을까?
        -   구글에서는 SRE라고 부르기도 한다.
        -   서버로 배포를 하는데, 이런 운영 과정을 엄청나게 오래 걸리도록 할 수 있다.
            -   요즘은 클라우드로 배포한다. 이미 플랫폼화 되어 있다.
        -   그러면, SRE팀은 뭘 하는가?
            -   개발된 서비스를 배포할 수 있는 플랫폼을 만드는 것이다.
            -   클라우드보다 좀 더 포장된 플랫폼을 만드는 것이다.
-   구글의 SRE 엔지니어는 다음과 같이 일한다.
    -   자동화 프로세스를 만든다.
        -   CI/CD 하고, 오토메이션 만들고, 프로메테우스 만들다.
        -   이렇게 자동화되면 다른 팀으로 옮겨가 자동화를 시작한다.
    -   구글 클라우드에서 나온 서비스들은 구글 내부에서 동일하게 사용한다.
-   플랫폼을 만들기 위해서는 이를 만들기 위한 기반이 필요한다.
    -   VM 기반은 표준이 없어 힘들다.
        -   MS Hyper-v / KVM / 기타등등 많다.
        -   무겁고, 라이센스비도 들다.
    -   컨테이너가 나왔고, 이건 가볍고 벤더 종속도 안탄다.
        -   컨테이너를 어디 배포할지 지정해야 한다.
            -   이걸 컨테이너 오케스트레이션 시스템이라고 한다.
        -   구글만 쿠베를 지원했고, AWS는 아파치 기반 / 애저는 스왐 뭐시기 기반이었고, 쿠베가 통일했다.
            -   쿠베는 이렇게 시작되었다.

## 컨테이너

-   일반적인 서버는 하드웨어, 서버, OS 종속성을 포함한다.
-   가상화 돌리면 hypervisor 돌리고, 새롭게 에뮬레이션 하여 커널 돌린다.
    -   여기에 OS 깐다.
    -   Virtual image 내부에 커널이 돌아간다.
        -   VM 밖을 벗어날 수 없다.
    -   Container는 커널 이미지가 원칙적으로는 들어가지 않는다.
        -   OS 의존성과 소스코드로만 돌아갈 수 있도록 하는 것이다.
        -   의존성을 포함하고, 프로세스를 독립시켜 작동시키는 것이다.
        -   그래도 커널은 루트의 커널이다.
            -   다른 OS가 올라가도 커널은 공유하게 된다.
            -   커널을 탈취하면 서버의 커널을 탈취한거다. 서버 뚫린거란 뜻이다.
-   예전에는 컨테이너로 다 소프트웨어 장비 넣는다.
    -   컨테이너 기술 사용하면 최적화 잘되고 벤더 독립성 있다.
    -   화웨이가 커널 탈취해서 트래픽 탈취하는 시나리오가 실제로 발생했다.
-   컨테이너는 하드웨어를 로드하기 위한 자원이 작다.
    -   보안적으로 vm보다 적다.

### 도커의 이미지 구조

-   도커의 이미지는 압축 파일로 구성된다.
    -   랩탑 / 서버라고 치자.
    -   이미지를 설치하면 도커허브에서 이미지를 카피해온다.
        -   이후 extract한다.
        -   이때 같은 이미지를 포함하고 있다면, 동일한 이미지는 같이 사용하다.
    -   레이어 형태로, 이미지를 쌓아가며 새로운 이미지를 만든다.
        -   도커 이미지를 설치하게 되면 서버에 캐싱이 된다.
            -   레이어 단위로 캐싱이 이루어지게 되고, 또 다른 파이썬 앱을 배포할 때 이미지를 재활용할 수 있게 된다.
-   쿠버네티스를 운영할 때, 컨테이너의 uptime을 중요시 해야한다.
    -   빠르게 로드될 수 있어야 한다.
    -   스케일 업을 하는데 존나 오래걸리면 아주 신나는거다.
        -   딥러닝이면 로드하는데만 30분씩 걸리는 신나는거다.
        -   이미지에 모델 자체를 구워놓는 방법이 있다.
    -   로딩 타이밍을 줄이기 위한 여러가지 방법이 있다.
        -   자바 이미지 서빙을 잘하면 300메가 - 못하면 1기가까지 나올 것이다.
    -   베이스 이미지를 생각해서 만드는 것이 빠르고 가볍게 서빙할 수 있게 되는 것이다.
        -   알파인 이미지등을 사용하면 더 빨라지게 되는 것이다.
        -   대부분의 시간은 압축을 푸는데 걸리기만 하는 되는거라, 시간을 아낄 수 있다.
-   자바로 개발하면 jvm 들어가고 스프링 들어가고 하이버네이트 / 서버 들어가게 된다.
    -   이걸 묶어서 배포하는데, 굳이 이걸 묶을 필요가 없다.
        -   도커 컨테이너는 묶여있는 걸 하나의 파일로 본다.
        -   공통되는 부분을 굳이 묶어서 보낼 필요가 없다.
-   도커 파일로 이미지를 만드는데, 요즘은 도커를 잘 안쓴다.
    -   컨테이너 종류가 많은데, 실제 런타임에서는 도커를 잘 사용하지 않는다.
    -   runc / devisor등의 다른 런타임을 사용하게 된다.
        -   이걸 하기 위해서 CRI라는 것을 사용한다.
            -   컨테이너인 척 하기 위해서 지켜야 하는 인터페이스이다.
            -   이러한 API 스펙만 맞추면 된다.
-   도커라이징을 안하고 빌드 타임에 도커 이미지를 만드는 패키지들이 있다.
    -   이런거 쓰면 된다.
    -   우분투 이미지를 지우고 새로운 이미지를 만드는 사람들도 있다.
        -   업데이트 뜨면 신나는거다.

## 쿠버네티스

### 쿠버네티스의 아키텍처

-   구글에서 보그라는 코드명으로 만들어졌다
    -   구글 클라우드가 쿠베 위에서 돈다.
        -   vm 띄우면 컨테이너 안에서 vm이 뜬다.
        -   아키텍처 구조적으로 보면 재밌다.
-   기본적으로 control plain 이라는 것이 있다.
    -   흔히 말하는 admin 서버들이 control plain이다.
        -   kubectl이라는 커맨드를 쓰고, UI를 사용할 수도 있는데, 외부에서 요청을 쿠베에서 받는게 API 서버가 된다.
        -   kubectl에서 create container를 만들면 데이터 플레인을 만들어야 한다.
            -   데이터 플레인에 서버가 여러개면? 이러한 서버들을 노드라 한다.
            -   노드는 VM이 될수도 있고 베어메탈이 될 수 있다.
                -   어떤 노드로 보내야 할까? Scheduler로 보내게 된다.
            -   A 서버에 배포를 하고, 컨테이너를 올리는데 이 명령어를 받는 에이전트가 Kubelet이다.
                -   Kubelet이 뜨면 컨테이너 런타임이 컨테이너를 실행시킨다.
                -   트래픽이 들어와야 하는데, 컨테이너가 여러개 돌고 있으면 어디로 보내야 할까?
                    -   트래픽을 관리해주는 컴포넌트가 kubeproxy이다.
                    -   외부에서 트래픽이 들어오면 프록시가 받아서 올바른 컨테이너로 보내게 된다.
        -   configuration 정보를 갖고 있어야 한다.
            -   ETCD라는 데이터베이스가 control plane이 있고, 여기에 저장되게 된다.
            -   hash table과 유사한 형태로 구성된다.
        -   오토 스케일링이나 크러시가 났을 때 이를 관리해주는 컴포넌트가 있다.
            -   이러한 컴포넌트를 컨트롤러라고 한다.
-   이후 Data plain이 들어간다.
    -   실제 워크로드가 여기서 돌아간다.
-   쿠버네티스는 여러개의 노드 / VM 위에서 돌아가는 분산형 아키텍처라 한다.
    -   실제 워커 노드를 돌리는 것을 data plane이라 한다.
-   커맨드를 받는 것은 마스터 노드의 api 서버가 담당한다.
    -   컨테이너를 호스팅해주는 역할을 하고 밖에서 들어오는 트래픽은 kubeproxy가 라우팅 해주는 역할을 한다.
    -   어디 서빙될지는 scheduler가 관리하고, api 서버등의 매니징한다.
    -   controller-manager 같은걸 batch job / auto scaling등의 동작을 한다.
    -   kube-scheduler등이 존재한다.
    -   kube-dns
        -   쿠베를 띄우면 노드들이 있고, 컨테이너들이 뜰거다.
            -   어떤 노드에 컨테이너가 뜨는지 모르고, ip address는 매번 바뀐다.
            -   ip로 포인팅하면 호출할 수 없다.
                -   서비스라는 개념이 생기게 된다.
                -   그렇기 때문에 DNS를 사용하게 되고, 따라서 쿠베 내장 DNS를 사용하게 된다.
    -   Pod로 직접 트래픽을 안보내고, 로드밸런서가 Pod 묶어서 트래픽을 보내게 된다.
        -   로드밸런서에 DNS를 두게 된다. 이를 서비스명이라 한다.
            -   DNS에 하면 서비스명이 부여가 된다.
        -   하나의 클러스터에 15000개의 노드를 붙일 수 있고, 한 개의 노드에서 256개의 컨테이너까지 붙일 수 있다.
            -   총 384만개의 컨테이너를 띄울 수 있다.
        -   클라우드의 자원에는 논리적 한계가 있다.
            -   하나의 아마존 서버에서 384만개를 주면 로드밸런스를 못주는 리소스 문제가 발생하게 된다.
                -   DNS가 터진다. 이거 조그마한 컨테이너로 뜨는거다.
                -   어느정도 규모 이상의 컨테이너는 설계 구조가 달라지게 된다.
                    -   벤더마다 클라우드 DNS가 따로 존재하고, 이를 사용하도록 config를 바꿔주어야 한다.
                -   컨테이너 노드 하나당 256개를 띄울 수 있고, 256개의 ip range를 물고 들어간다.
                    -   50개를 띄운다고 해도 256개를 먹는다.
                    -   조그마하게 만드는 일반적인 클러스터도 디자인이 바뀌어야 한다.
                        -   1000단위 노드는 그냥 놔두지도 않고, 비싼 엔지니어를 고용하게 된다.
                        -   시스템 몰리기 시작하면 아키텍처 컨설팅을 하고, 잠재적인 문제를 분석하는 일을 하는 사람들이 있다. 내가 한다.
                    -   랜덤으로 ip가 지정되고, 연속성의 Ip가 지정되기 어렵다. ip range를 지정하기도 어렵다.
                        -   자기가 가지고 있는 ip를 클라우드에 등록해서 사용하는 방법이 있고, 이를 설계한다.
                        -   스타트업 들어가도 맨땅에 대가리 받지 말고, 담당 엔지니어 있다.
                            -   optimize 해야합니다. 이런 걸 얘기해주는 건 프로젝트 할 때 먼저 가서 얘기하라는 뜻이다.
                    -   CDN같은게 이런 케이스에 많이 들어간다.
                        -   멀티 CDN 계약 전략을 하는 등으로 비용을 절약할 수 있다.
                        -   여러개의 워크로드를 전환할 수 있도록 하고, 재계약때 갈아치우는 방법을 사용하면 좋다.
                            -   벤더 락인 걸리면 안된다. 머리 쓰면 좋은 방법이 많다.
                            -   서버리스를 람다만 쓰라는 법은 없고, 여러가지 서비스를 사용할 수 있다.
                -   여러개의 클라우드 서비스에 쿠베가 있을때 ISTIO를 띄워서 가상화 네트워크를 만들고, 엔드포인트로 이걸 로드밸런싱을 하면, 클라우드별로 분류할 수 있는 것이다.
                    -   내 맘대로 트래픽을 보낼 수 있는 것이다.
                    -   이정도면 상당한 고수다.
                        -   마이크로 서비스가 트래픽을 자동으로 잡는다.
                        -   트래픽 라우팅을 하고, 카나리 테스팅을 MSA를 통해 할 수 있는 것이고, 에러를 실시간으로 모니터링 할 수 있는 것이다.
            -   대시보드
                -   쿠베 대시보드 안쓴다.
                    -   UI인데, 요즘은 좀 괜찮은데 예전에는 인증 기능이 없었다.
                        -   OAuth등의 기능을 써야 하는데, 인증 제대로 작동 안되면 비트코인 채굴장 되는거다.
                -   쿠베 기술중에 binary auth 기능이 있다.
                    -   컨테이너 이미지 아무거나 써서 문제 생기는 경우 많다.
                    -   도커 이미지 아무거나 찾아 쓰면 지옥갑니다.
                        -   가이드를 해도 신입은 지좆대로 한다. 그래서 공인된 이미지만 사용하도록 강제할 수 았다.
                        -   개발은 편하게, 정책을 강하게.

### 쿠버네티스
#### Pod
-   쿠버네티스의 최소 배포 단위
	-   컨테이너가 아니다.
	
-   팟은 한개 이상의 컨테이너로 구성된다.
	-   하나씩 컨테이너 배포 안하고 왜 이렇게 하지?
		-   어플리케이션을 보통 배포하면 어플리케이션 하나만 나이스하게 배포하지 않는다.
		-   node.js 어플 배포하려면 기본적으로 로깅 툴 / 모니터링 에이전트 붙는다.
			-   이걸 하나로 묶으면, 패키지 하나씩 바꿀때마다 신나진다.
			-   편차가 심하다. 운영하는 입장에서 통일해야 한다
				-   이런 경우 이런것들을 잘라서 배포하면 된다.
				-   CI / CD는 컨테이너 매번 새로 배포해야 한다.
			-   모니터링 / 로깅 툴을 분리하여 컨테이너로 설계하면 어떨까?
				-   운영 팀이 모니터링 / 로깅을 책임, 어플리케이션은 개발 팀이 책임지는 방식으로 작동한다.
			-   이걸 사이드카 패턴이라 한다.
				-   굉장히 널리 쓰이는 패턴이다.
				-   프록시 등 인젝션할 때 자주 사용된다.
				
-   ip address와 disc를 공유한다.
-   쿠베의 리소스들의 포드 등에는 레이블을 달 수 있다.
	-   일종의 식별자인데, 어플리케이션을 배포하고 프론트라인 / 백엔드 / 데이터베이스가 존재한다.
		-   프론트엔드의 CP만 보고싶을 때, 태그만 가지고 배포할 수 있을 것이다.
		-   프론트엔드 서버에만 롤을 주는 등의 선택을 하도록 할 수 있다.
	-   방화벽 규칙 등에 활용할 수 있다.
		-   내부 트래픽 통제를 위해서 태그를 사용할 수 있다.
			-   ip를 강제할 수 없기 때문에 태그를 지정한다.
	-   태그는 한 개 이상을 작성한다.
		-   이런 식으로 레이블을 사용하게 된다.
-   어플리케이션 돌릴때 컨테이너를 여러개 띄운다.
    -   템플릿 식으로 pod를 여러개 띄우는 것이 replication controller이다.
        -   api 서버 있고 이거 관리하는 컨트롤러이다. 그 중 첫번째가 지금 애기하는 replication controller이다.
            -   replica를 몇개 띄울지 결정해준다.
            -   레이블에 매칭이 될 때 까지 replication이 띄운다.
        -   템플릿에 의해 배포되지 않았아도 레이블 수만 맞으면 된다.
        -   replication controller에 의해 포드를 계속 띄우게 된다.
            -   다른 사람이 추가로 띄워도 그것도 띄운걸로 치다.
        -   어플리케이션 내용이 어떤지 상관 없이 레이블이 만든대로 로드밸런싱을 한다.
            -   카나리 테스트 등을 이러한 방식으로 적용할 수 있다.
        -   프론트엔드 입장에선 앞의 레이블만 확인하고, 따라서 이에 따라 달라지게 되는 것이다.
    -   replica set
        -   replication controller와 동일한데, 레이블을 선택하는 방식이 다르다.
        -   replica set은 집합을 사용한다.
            -   레이블의 셋을 묶어서 집합으로 띄우게 된다
            -   canary deployment등의 고급 기능을 필요로 한다.
#### Deployment
-   버전 2를 업그레이드하고 싶으면, 새로운 컨테이너를 띄우고 로드밸런스로 스위칭 할 수 있다.
	-   이렇게 작동하면 리소스가 6개 필요하고 부족할 수 있다.
-   기존거를 하나씩 지우고 새로 띄우는 방식으로 업데이트 할 수 있다.
	-   롤링 업데이트라 한다.
	-   deployment라는 것은 기능을 많이 필요로 한다.
-   Service
	-   Selector에 따라서 로드밸런싱을 진행한다.
	
-   여러가지 pod가 뜨면 하나로 묶어서 내는 것을 서비스라고 한다.	
	-   이정도면 api server는 만들 수 있다.
