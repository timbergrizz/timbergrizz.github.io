오늘부터 데일리 엔지니어링 노트는 velog에 올리려고 한다. 툴을 여러개 쓰는걸 별로 좋아하진 않지만, Toggle? 리스트식으로 항상 글을 쓰기보다는 문장 형식으로 글을 구성하는 연습을 velog에 올리는 글을 통해 해보고자 한다. 리스트형식으로 글을 쓰는게 나쁘다기보단, 조금 더 논리적으로 글을 쓸 수 있어야 더 멀쩡한 글을 쓸 수 있을 것 같다.

## 컬리 면접 준비
컬리 면접 준비하면서 계속 이런저런 ML 플랫폼과 관련된 글을 읽으면서 느끼는 점은, 생각보다 데이터 엔지니어와 데이터 사이언티스트의 역할이 다르다는 점이다. 데이터 엔지니어는 어떻게 생성되는 데이터를 잘 **처리**할 수 있는가에 집중되는 느낌이고, 사이언티스트는 어떻게 데이터로부터 의미있는 **가치**를 뽑아낼 수 있는가에 관한 얘기인 것 같다.

[쿠팡 로켓그로스의 ML 플랫폼: 20개 이상의 모델 서비스 및 트래픽 처리 비용 효율화](https://medium.com/coupang-engineering/%EC%BF%A0%ED%8C%A1-%EB%A1%9C%EC%BC%93%EA%B7%B8%EB%A1%9C%EC%8A%A4%EC%9D%98-ml-%ED%94%8C%EB%9E%AB%ED%8F%BC-20%EA%B0%9C-%EC%9D%B4%EC%83%81%EC%9D%98-%EB%AA%A8%EB%8D%B8-%EC%84%9C%EB%B9%84%EC%8A%A4-%EB%B0%8F-%ED%8A%B8%EB%9E%98%ED%94%BD-%EC%B2%98%EB%A6%AC-%EB%B9%84%EC%9A%A9-%ED%9A%A8%EC%9C%A8%ED%99%94-f8f362ea71fc)
로켓 그로스 서비스는 아마존처럼 쿠팡에서 풀필먼트를 제공하는 사업인 것 같다. 서비스가 커지는 과정에서 상품 확인등의 수동으로 진행되는 프로세스가 병목으로 작용했고 이를 ML 기술을 통해 대체하고자 했으며 이러한 맥락에서 ML 아키텍처를 새롭게 구축하게 된 것 같다.

최초의 모델은 카프카를 통한 ML 파이프라인이었고, **비동기적 파이프라인** 구성을 통한 지연시간 문제에 대한 자유와 모듈간 분리를 통한 **컴포넌트간의 느슨한 결합**이 장점이었다. 하지만 이러한 구조는 새로운 feature가 추가되면 해당 feature와 관련된 모든 카프카 토픽에 producing 로직과 응답 집계를 **직접 구현해야** 했으며, 모델이 추가될수록 카프카에 셋업을 **직접 해야하여** 개발 과정에 있어 복잡도가 증가하였다. 또한, 카프카 토픽에 producing / consuming되는 과정을 필요로 하여 추가적인 지연시간이 발생하였다.

이러한 아키텍처를 개선하기 위해 ML 모델과 클라이언트 간의 **오케스트레이션 계층**을 도입하였고, REST 서비스로 **모델과 상호작용**하는 모델을 만들었다. 오케스트레이션 계층이 요청을 각각의 모델로 라우팅하고, 요청에 대한 응답이 집계되어 클라이언트에 반환된다. 이를 통해 직접 카프카 컨슈머 / 퍼블리셔를 설계하는 과정을 수행하지 않아도 처리되게 되었다. ML도 현재는 쿠버네티스를 바탕으로 파이프라인을 구성하고, 서비스를 설계하는 것이 많은 것 같다.

또한 자원 관리도 GPU를 필요로 하는 복잡한 컴퓨터 비전 등의 모델과 태그 인식등의 간단한 모델로 나누어 **서빙을 다르게 처리**하도록 한 후, 이에 맞춰 프로비저닝 되도록 하여 비용을 절약하면서 최선의 성능을 낼 수 있도록 하였다. GPU 자원은 비싸고, 이러한 자원 관리에서의 효율성을 통해 비용을 절약하며 최대한의 성능을 낼 수 있었다고 한다. 비용을 절약하기 위한 여러가지 전략을 확인할 수 있었다.

어쨌거나 ML 모델을 서비스에 사용함에 있어서 무지성으로 클라우드 자원을 끌어다 쓰기 보다는, 자원의 효율성 측면과 처리 속도를 모두 개선시킬 수 있는 방법으로 생각하는게 좋을 것 같다.


[리디의 머신러닝 파이프라인 톺아보기](https://ridicorp.com/story/machine-learning-pipeline/)

리디의 경우 데이터를 처리하기 위해 EMR을 이용하고 있으며, 파이프라인의 워크플로우를 Airflow를 이용해 스케줄링 하고 있다. EMR의 경우는 하둡 프레임워크로, 하둡 기반의 기술을 이용하여 데이터를 정제하고 분석 처리할 수 있다. Redshift는 이미 ETL이 이루어젠 데이터를 대상으로 한다면, EMR은 원천 데이터에 대한 정제와 분석 처리를 할 수 있다는 점이 차이가 될 것 같다.

EMR과 Athena, Redshift 모두 아마존의 서비스이지만 서로 다른 요구사항에 따라 처리가 이루어지고, 적절한 도구를 선택하는 것이 중요할 것 같다. 자세한 차이점은 이 링크를 통해서 확인해보면 좋을 것 같다.
[아마존 Redshift와 EMR의 차이 - hs_seo](https://118k.tistory.com/638)

모델 개발을 위해서는 Bazel이라는 툴을 사용하고 있고, 이를 통해 빌드하면 자동으로 EKS에 배포되어 학습 모델과 추론 결과를 만들 수 있다고 한다. 이것도 역시 쿠버네티스 기반이다. 이러한 솔루션을 통해 로컬에서 EKS를 통해 모델을 실험할 수 있고, stage나 production 환경에서 모델을 배포하는 과정도 GitOps를 이용해 처리할 수 있다고 한다.

이렇게 머신러닝 학습을 수행할 수 있으면서 EKS의 컨테이너를 효율적으로 운영하기 위해서 Karpenter를 사용하고 있으며, 작업이 실행되는 동안만 GPU가 장착된 인스턴스를 프로비저닝하여 사용하고 사용이 끝나면 자동으로 반환되도록 한다고 한다. 클라우드 기반의 서비스들은 이러한 최적화도 굉장히 중요한 챌린지가 되는 것 같다. 이러한 지점에서 어떠한 방식으로 비용을 아낄 수 있을지 배워보면 좋을 것 같다.

모니터링은 Datadog으로 수집하여 대시보드로 모니터링 하고, Slack과 연결하여 모니터링이 진행될 수 있도록 한다고 한다. 또한 Airflow의 워크플로우의 성공 여부 등도 Datadog으로 수집해 관리할 수 있다고 한다. Datadog이 굉장히 비싼 솔루션으로 알고 있는데, 두가지 의문점이 드는건 그만큼 사용하기 좋은가? / 왜 다른 클라우드 벤더는 그만큼 경쟁력 있는 모니터링 솔루션을 만들지 못하는가정도 인 것 같다.

서빙 관련해서는 Node.js를 이용해 추론 결과를 제공한다고 하는데, 어떻게 구성되는지를 모르겠다. 보통은 파이썬에 TFServing 아니면 TorchServe 붙여서 처리하는 쪽이나 파이프라인 설계로 처리하는 쪽이 많은걸로 아는데, 카프카같은걸로 구성해서 연결하는건가 싶다.



